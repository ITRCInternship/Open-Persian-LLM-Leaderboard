# Research on classifying Persian texts (tweets/comments) with LLM and their results.
# چکیده

   با گسترش شبکه‌های اجتماعی، تولید محتوای متنی در زبان فارسی به‌ویژه در قالب نظرات و توییت‌ها افزایش یافته است. طبقه‌بندی خودکار این متون می‌تواند نقش مهمی در تحلیل احساسات، شناسایی محتوای توهین‌آمیز و تشخیص اسپم ایفا کند. در این پژوهش، به بررسی عملکرد مدل‌های زبانی بزرگ (LLM) در طبقه‌بندی متون فارسی می‌پردازیم. 
  مدل‌هایی مثل ParsBERT، XLM-R و mBERT  به‌صورت دقیق ارزیابی و مقایسه شده‌اند. نتایج نشان می‌دهد که مدل‌های زبان‌بومی‌شده مانند ParsBERT نسبت به مدل‌های چندزبانه دقت بالاتری در درک متون غیررسمی فارسی دارند.
  
  # مقدمه:
  
  با رشد روزافزون استفاده از شبکه‌های اجتماعی و پیام‌رسان‌ها، حجم عظیمی از محتوای متنی به زبان فارسی به‌صورت روزانه تولید می‌شود. تحلیل این داده‌ها برای اهدافی همچون تحلیل احساسات کاربران، شناسایی نظرات مخرب، تشخیص نظرات جعلی یا تبلیغاتی، و دسته‌بندی موضوعی اهمیت بسیاری یافته است. از طرفی، زبان فارسی به دلیل ساختار خاص خود، چالش‌هایی برای پردازش زبان طبیعی (NLP) ایجاد می‌کند.
  مدل‌های زبانی بزرگ (Large Language Models) یا LLM) مانند BERT و GPT تحولی عظیم در حوزه پردازش زبان طبیعی ایجاد کرده‌اند. با آموزش این مدل‌ها روی داده‌های عظیم و استفاده از معماری‌های عمیق، امکان فهم بهتر ساختار و معنای جملات فراهم شده است. در سال‌های اخیر، مدل‌هایی خاص برای زبان فارسی نیز توسعه یافته‌اند که از مهم‌ترین آن‌ها می‌توان به ParsBERT اشاره کرد.
  در این تحقیق، هدف ما بررسی و مقایسه کارایی مدل‌های LLM در طبقه‌بندی متون فارسی است. تمرکز اصلی روی داده‌های شبکه‌های اجتماعی مانند توییتر و نظرات کاربران است که اغلب دارای لحن غیررسمی، نگارش متفاوت و واژگان عامیانه هستند. در سال‌های اخیر، استفاده از مدل‌های زبانی بزرگ (LLM) برای پردازش زبان طبیعی (NLP) در زبان فارسی، رشد چشم‌گیری داشته است. در این بخش، مهم‌ترین مدل‌های زبانی مورد استفاده در طبقه‌بندی متون فارسی بررسی می‌شوند.

   
   #  1)	ParsBERT


•	توسعه‌دهنده: دانشگاه علم و صنعت ایران (2020)

•	پایه: معماری BERT

•	ویژگی: آموزش‌دیده بر بیش از 3 میلیارد کلمه از منابع فارسی (ویکی‌پدیا، اخبار، کتاب‌ها)
•	کاربرد: تحلیل احساسات، طبقه‌بندی متون، پاسخ‌ به پرسش‌ها
•	مزیت: عملکرد بهتر از mBERT در داده‌های فارسی
در پژوهش "ParsBERT: Transformer-based Model for Persian Language Understanding" نشان داده شد که این مدل در اکثر تسک‌های NLP فارسی بهتر از مدل‌های چندزبانه عمل می‌کند.



# 2)	mBERT (Multilingual BERT)

•	توسعه‌دهنده: Google 

•	زبان‌ها 104: زبان از جمله فارسی

•	ویژگی: آموزش دیده به‌صورت چندزبانه بدون تنظیم خاص برای زبان فارسی

•	مزیت: مناسب برای زبان‌هایی با داده کم

•	نقص: عملکرد ضعیف‌تر از ParsBERT در فارسی‌های غیررسمی و عامیانه



# 3)	XLM-RoBERTa
•	توسعه‌دهنده Facebook AI (Meta):

•	زبان‌ها: چندزبانه با عملکرد بهتر از mBERT در زبان‌های کم‌منبع

•	ویژگی: مبتنی بر RoBERTa و دارای قدرت تعمیم بهتر

•	در فارسی: نسبتاً بهتر از mBERT، ولی هنوز از ParsBERT ضعیف‌تر




# 4)	GPT-3 / GPT-4 (OpenAI) 

•ویژگی: مدل‌های مولد با توانایی فوق‌العاده در درک زبان

•	محدودیت: آموزش‌ندیده مستقیماً روی داده‌های فارسی؛ نیاز به ترجمه برای استفاده دقیق

•	روش پیشنهادی: Prompting همراه با ترجمه فارسی به انگلیسی

•	نتیجه: دقت قابل قبول ولی محدودیت در فهم ظرافت‌های زبانی فارسی






# 5)	ParsiGPT ParsiGPT و Ghazal مدل‌های GPT فارسی

•	توسعه‌دهنده: آزمایشگاه NLP دانشگاه تهران

•	معماری: مشابه GPT اما با داده‌های فارسی

•	کاربرد: پاسخ به پرسش، تولید متن، طبقه‌بندی ساده

•	محدودیت: مقیاس کوچک‌تر، داده آموزشی محدود، در مرحله آزمایشی

# مقایسه مدل‌های زبانی برای فارسی

در جدول زیر ویژگی‌های برخی مدل‌های زبانی مناسب برای پردازش زبان فارسی مقایسه شده‌اند:




# داده‌ها و پیش‌پردازش

## مجموعه داده

از دیتاست توییت‌های فارسی شامل سه دسته برچسب‌خورده استفاده شده است:

•	Normal  عادی

•	 Offensive توهین‌آمیز

•	 Hate نفرت‌پراکن




# مراحل پاک‌سازی

•	حذف لینک‌ها، کاراکترهای خاص، ایموجی‌ها و اعداد

•	نرمال‌سازی کلمات


•	توکن‌سازی با استفاده از Tokenizer مدل مورد نظر مثلاً ParsBERT 




                                                                                                                      
                                                                                                                               




# Open-Persian-LLM-Leaderboard


# 1.	Model = مدل

 نام مدل زبان بزرگ (مانند llama-3, gemma, qwen, aya و ...) که مورد ارزیابی قرار گرفته است.
 
✅ مقایسه بین مدل‌هاست، نه عددی.

________________________________________

# 2.	Precision = دقت عددی


نوع دقت عددی مورد استفاده برای اجرا یا آموزش مدل است، مثل bfloat16 یا float16. این بر حافظه و سرعت تأثیر دارد.

✅ تأثیر مستقیم بر عملکرد ندارد، بیشتر فنی است.

________________________________________
# 3.	Params (B) = تعداد پارامترها بر حسب میلیارد


 اندازه مدل بر حسب میلیارد پارامتر.
 
📌 مدل‌های بزرگ‌تر معمولاً توانمندترند ولی منابع بیشتری مصرف می‌کنند.

✅ بیشتر = توانمندتر، اما منابع بیشتر نیاز دارد

________________________________________

# 4.	Average Accuracy = دقت میانگین


میانگین دقت مدل در تمام بنچمارک‌هایی که در جدول آمده است.

✅ بیشتر بهتر است.

________________________________________

# 5.	Part Multiple Choice

درصد موفقیت مدل در بخش سؤالات چندگزینه‌ای (احتمالاً درک مطلب، منطق و اطلاعات عمومی.)

✅ بیشتر بهتر است.
________________________________________

# 6.	ARC Easy

عملکرد مدل در نسخه آسان آزمون ARC (AI2 Reasoning Challenge) که تست منطق و دانش ابتدایی است.

✅ بیشتر بهتر است.
________________________________________

# 7.	ARC Challenge

عملکرد در نسخه سخت‌تر ARC. این نسخه از مدل، استدلال پیچیده‌تری می‌طلبد.

✅ بیشتر بهتر است.

________________________________________

# 8.	MMLU Pro

میانگین دقت مدل در آزمون MMLU (Massive Multitask Language Understanding) با سطح حرفه‌ای، شامل موضوعاتی مانند ریاضیات، حقوق، پزشکی، و ...

✅ بیشتر بهتر است.
________________________________________

# 9.	AUT Multiple Choice Persian

عملکرد مدل در تست چندگزینه‌ای به زبان فارسی (احتمالاً آزمون از دانشگاه صنعتی امیرکبیر).

✅ بیشتر بهتر است.



# 1)meta-llama/Llama-3.3-70B-Instruct


•Precision = bfloat16

•Params (B) = 70.6

•Average Accuracy = 70.364

•Part Multiple Choice = 51.42

•ARC Easy = 95.4

•ARC Challenge = 89.93

•MMLU Pro = 43.67

•AUT Multiple Choice Persian = 71.4


# 2) google/gemma-3-27b-it


•Precision = bfloat16

•Params (B) = 27.4

•Average Accuracy = 67.85

•Part Multiple Choice = 48.56

•ARC Easy = 95.69

•ARC Challenge = 90.6

•MMLU Pro = 40.1

•AUT Multiple Choice Persian = 64.3


# 3) google/gemma-2-27b-it

•Precision = bfloat16

•Params (B) = 27.2

•Average Accuracy = 65.464

•Part Multiple Choice = 46.03

•ARC Easy = 95.98

•ARC Challenge = 85.91

•MMLU Pro = 36.28

•AUT Multiple Choice Persian = 63.12


# 4) Qwen/QwQ-32B-Preview

•Precision = bfloat16

•Params (B) = 32.8

•Average Accuracy = 64.784

•Part Multiple Choice = 46.64

•ARC Easy = 91.95

•ARC Challenge = 87.24

•MMLU Pro = 37.94

•AUT Multiple Choice Persian = 60.15

# 5) Qwen/Qwen2.5-32B-Instruct

•Precision = bfloat16

•Params (B) = 32.8

•Average Accuracy = 64.46

•Part Multiple Choice = 46.06

•ARC Easy = 90.8

•ARC Challenge = 85.91

•MMLU Pro = 38.19

•AUT Multiple Choice Persian = 61.34

# 6) google/gemma-2-9b-it

•Precision = bfloat16

•Params (B) = 9.24

•Average Accuracy = 62.886

•Part Multiple Choice = 42.7

•ARC Easy = 93.1

•ARC Challenge = 84.56

•MMLU Pro = 31.74

•AUT Multiple Choice Persian = 62.33

# 7) CohereLabs/aya-expanse-32b

•Precision = float16

•Params (B) = 32.3

•Average Accuracy = 61.938

•Part Multiple Choice = 43.36

•ARC Easy = 93.1

•ARC Challenge = 79.87

•MMLU Pro = 31.03

•AUT Multiple Choice Persian = 62.33















